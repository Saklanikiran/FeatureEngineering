{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1d9d6c-be19-496a-b17a-4bc595576c4d",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbb33b0-c10c-4d77-a992-9d39d1fd77ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Min-Max scaling : Min-Max scaling is a data preprocessing technique used to scale numeric features to a specific range, typically between 0 and 1.\n",
    "            It transforms the data such that the minimum value becomes 0, the maximum value becomes 1, and all other values are scaled proportionally \n",
    "            in between.\n",
    "            \n",
    "            \n",
    "Formula of min-max scalling : \n",
    "                            X(scaled)=(X-X(min))/(X(min)-X(max))\n",
    "                            X: original value\n",
    "                            \n",
    "'''\n",
    "# Example : We want to scale this age value using Min-Max scaling to a range between 0 and 1.\n",
    "def min_max_scaling(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    scaled_data = [(x - min_val) / (max_val - min_val) for x in data]\n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "data = [20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "scaled_data = min_max_scaling(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc0180-f6b8-4453-be15-9499d0d82a35",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8005213f-6b6a-4316-a675-12e1c655d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[160  60]\n",
      " [170  70]\n",
      " [180  80]\n",
      " [190  90]]\n",
      "\n",
      "Scaled data (unit vectors):\n",
      "[[0.93632918 0.35112344]\n",
      " [0.9246781  0.38074981]\n",
      " [0.91381155 0.40613847]\n",
      " [0.90373784 0.42808634]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Unit vector technique : The Unit Vector technique, also known as vector normalization, is a feature scaling method that scales the values of features\n",
    "    to have a unit norm, typically L2 norm (Euclidean norm). It transforms the values of each feature such that the magnitude of the feature vector becomes\n",
    "    1. This technique is particularly useful when the direction of the feature vectors matters more than their actual magnitude.\n",
    "\n",
    "The formula for unit vector normalization (L2 normalization) is:\n",
    "\n",
    "Unit Vector=Feature Vector/∥Feature Vector∥2\n",
    "\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "Feature Vector : Feature Vector is the vector of feature values.\n",
    "∥Feature Vector∥2 : is the L2 norm of the feature vector, calculated as the square root of the sum of the squares of its elements.\n",
    "\n",
    "\n",
    "Unit Vector scaling differs from Min-Max scaling in that it doesn't scale the values to a specific range like Min-Max scaling does. Instead, \n",
    "it focuses on preserving the direction of the feature vectors while making their magnitude uniform.\n",
    "\n",
    "'''\n",
    "# Example : \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def unit_vector_scaling(data):\n",
    "    norms = np.linalg.norm(data, axis=1, ord=2)  \n",
    "    scaled_data = data / norms[:, np.newaxis]   \n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "data = np.array([[160, 60], [170, 70], [180, 80], [190, 90]])\n",
    "\n",
    "\n",
    "scaled_data = unit_vector_scaling(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled data (unit vectors):\")\n",
    "print(scaled_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873380a-e475-4019-9c63-f64a520973dc",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47547660-e057-4a63-bbed-abecf4dddd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[160  60]\n",
      " [170  70]\n",
      " [180  80]\n",
      " [190  90]]\n",
      "\n",
      "Transformed data (after PCA):\n",
      "[[ 21.21320344]\n",
      " [  7.07106781]\n",
      " [ -7.07106781]\n",
      " [-21.21320344]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in data analysis and machine learning. It aims to \n",
    "reduce the number of features (or dimensions) in a dataset while preserving the most important information. PCA achieves this by transforming the\n",
    "original features into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "\n",
    "The main steps of PCA are as follows:\n",
    "    1. Standardize the data: If the features of the dataset are on different scales, it's essential to standardize them to have a mean of 0 and a standard deviation of 1.\n",
    "    2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data, which represents the relationships between the features.\n",
    "    3. Compute eigenvectors and eigenvalues: Determine the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of the new feature space, while eigenvalues represent the magnitude of the variance explained by each eigenvector.\n",
    "    4. Select principal components: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues (variance) are the principal components.\n",
    "    5. Project the data onto the new feature space: Transform the original data onto the new feature space formed by the selected principal components.\n",
    "\n",
    "PCA is used in various applications, including data visualization, noise reduction, and feature extraction for machine learning algorithms.\n",
    "\n",
    "'''\n",
    "\n",
    "# Example:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[160, 60], [170, 70], [180, 80], [190, 90]])\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nTransformed data (after PCA):\")\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6c908-7f07-4f93-adce-af464a49e320",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c032da6-04b2-4a53-9691-859d6b1745dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[  160    60    25 40000]\n",
      " [  170    70    30 50000]\n",
      " [  180    80    35 60000]\n",
      " [  190    90    40 70000]]\n",
      "\n",
      "Transformed data (after PCA feature extraction):\n",
      "[[-1.50000169e+04  2.44042763e-15]\n",
      " [-5.00000562e+03 -1.03763677e-15]\n",
      " [ 5.00000562e+03  1.03763677e-15]\n",
      " [ 1.50000169e+04  1.74866978e-15]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA transforms the original features into a new set of orthogonal features \n",
    "called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance \n",
    "they explain in the data. Therefore, PCA effectively extracts the most important information from the original features and represents it in a \n",
    "reduced-dimensional space.\n",
    "\n",
    "PCA can be used for feature extraction:\n",
    "    1. Standardize the data: If necessary, standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "    2. Apply PCA: Compute the principal components of the dataset using PCA. This involves calculating the covariance matrix of the standardized data,\n",
    "        computing the eigenvectors and eigenvalues of the covariance matrix, and selecting the principal components based on their corresponding eigenvalues.\n",
    "    3. Select the desired number of principal components: Determine the number of principal components to retain based on the amount of variance\n",
    "        explained or the desired dimensionality of the reduced dataset.\n",
    "    4. Project the data onto the new feature space: Transform the original data onto the new feature space formed by the selected principal components.\n",
    "    \n",
    "'''\n",
    "\n",
    "# Example : \n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[160, 60, 25, 40000], \n",
    "                 [170, 70, 30, 50000], \n",
    "                 [180, 80, 35, 60000], \n",
    "                 [190, 90, 40, 70000]])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nTransformed data (after PCA feature extraction):\")\n",
    "print(transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f68a85-1bd5-4b51-be7c-236a7a01532d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78f1dd-9bce-400e-90a0-be2f1592c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To use PCA (Principal Component Analysis) for reducing the dimensionality of the dataset in a stock price prediction project, follow these steps:\n",
    "    1. Data Preprocessing: Ensure that the dataset is cleaned and standardized. This may involve handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "    2. Apply PCA: Use PCA to transform the high-dimensional feature space into a lower-dimensional space while preserving the most important information.\n",
    "    3. Select Number of Components: Decide on the number of principal components to retain based on the amount of variance explained or the desired dimensionality reduction.\n",
    "    4. Model Training: Train your stock price prediction model using the reduced-dimensional dataset.\n",
    "    \n",
    "\n",
    "By reducing the dimensionality of the dataset with PCA, you can potentially improve the computational efficiency of your model training process while\n",
    "retaining the most important information for predicting stock prices. Adjust the parameters of PCA, such as the number of components or the explained \n",
    "variance ratio, based on your specific requirements and the characteristics of the dataset.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213f5f9-af97-4d41-8694-c6002ef1b8ff",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38b312e-947a-40ed-bf5b-e4231bdae5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Scaled data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "\n",
    "scaled_data = ((data - data_min) / (data_max - data_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46245a55-e402-4fb7-8aa2-20b9cf912deb",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b3c82-80a1-4117-a560-a01c7b9bf139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
